---
title: "Interpretation with LIME of classification in high-dimensional tabular data"
date: "2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\  

\  

\  

\  


## Introduction

TODO

\  

\  


## Data Description

```{r include=FALSE}
predictors = read.csv(file = 'data.csv')
labels = read.csv(file = 'labels.csv')[,2]
```

The dataset was uploaded to Kaggle by/thanks to UCI Machine Learning Repository:  
https://www.kaggle.com/murats/gene-expression-cancer-rnaseq  

Source: Samuele Fiorini, samuele.fiorini@dibris.unige.it, University of Genoa, redistributed under Creative Commons license.  

The data consists of 801 patients.  

The 20532 independent variables are all RNA sequencing gene expression levels measured by a sequencing platform (Illumina HiSeq).  

The dependent categorical variable represent primary tumors occurring in different parts of the body, covering 5 tumor types including:  

* lung adenocarcinoma (LUAD)  
* breast carcinoma (BRCA)  
* kidney renal clear-cell carcinoma (KIRC)  
* colon adenocarcinoma (COAD)  
* prostate adenocarcinoma (PRAD) 

A machine learning model will be trained to predict the type of tumor for new patients.  
TODO: buscar un dataset similar pero que contenga tambien pacientes sanos.  





\newpage  







## Data Analysis and Preprocessing


```{r include=FALSE}
# keras expects the first category to be 0
# 0 BRCA
# 1 COAD
# 2 KIRC
# 3 LUAD
# 4 PRAD
cancer = as.integer(labels) - 1
```

\  

Absolute frequencies of the response:

```{r echo=FALSE, fig.width=4, fig.height=4, fig.align="center"}
barplot(table(labels), col=rainbow(15, s=0.5), ylab="Absolute frequencies", xlab="Cancer Type")
```

\  

```{r include=FALSE, fig.width=4, fig.height=3, fig.align="center"}
# removing sample name column
genes = predictors[,-1]

# a few columns only contain 0 values, these are removed
genes = genes[,colSums(genes != 0) > 0]
```


268 gene expression variables contain only 0s are removed.  

From the remaining 20264 gene expressions, 670 have at least 95% of 0s:

```{r echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
#table(stat_0s > 0.95)

stat_0s = c()
for(i in 1:ncol(genes))
  stat_0s = c(stat_0s, round(length(genes[,i][genes[,i]==0])/nrow(genes),3))

plot(density(stat_0s), main = "", xlab = "% of 0s")
```

\  

The predictors don't follow distributions close to normal. Nornality cannot be assumed:

```{r echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
# Kolmogorov-Smirnov (K-S) normality test and Shapiro-Wilk’s test.

stat_shap = c()
for(i in 1:ncol(genes))
  stat_shap = c(stat_shap,shapiro.test(genes[,i])$p.value)

plot(density(stat_shap), xlim=c(0,0.1), main = "Shapiro-Wilk’s test", xlab = "p-value")
```

\  

Symmetry and kurtosis:

```{r echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
library(moments)


stat_skew = c()
for(i in 1:ncol(genes))
  stat_skew = c(stat_skew, skewness(genes[,i]))

plot(density(stat_skew), main = "", xlim=c(-10,10), xlab = "skewness")
abline(v=0, lty=2) #normal


stat_kurt = c()
for(i in 1:ncol(genes))
  stat_kurt = c(stat_kurt, kurtosis(genes[,i]))

plot(density(stat_kurt), main = "", xlim=c(0,50), xlab = "kurtosis")
abline(v=3, lty=2) #normal
```

\  

The distribution of means suggests there are two categories of gene expression:

```{r echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
stat_mean = c()
for(i in 1:ncol(genes))
  stat_mean = c(stat_mean, mean(genes[,i]))

plot(density(stat_mean), main = "", xlab = "mean")
```

\  

Outliers:

TODO

```{r eval=FALSE, include=FALSE}
#library(rAverage)

fun <- function(x){
   quantiles <- quantile( x, c(.01, .99 ) )
   newvalues <- quantile( x, c(.05, .95 ) )
   x[ x < quantiles[1] ] <- mean(x)#newvalues[1]
   x[ x > quantiles[2] ] <- mean(x)#newvalues[2]
   x
}
```

```{r eval=FALSE, include=FALSE, fig.width=4, fig.height=3, fig.align="center"}
stat_sd = c()
for(i in 1:ncol(genes))
  stat_sd = c(stat_sd, sd(genes[,i]))

plot(density(stat_sd), main = "", xlab = "sd")
```

\  

The data is scaled to help with the training of the prediction model.  

```{r include=FALSE}
genes = scale(genes)
```

At least 45% of the variability of the data is explained by the first 10 PCA components:  

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=5, fig.height=4, fig.align="center"}
library(factoextra)
genes_pca = prcomp(genes)
fviz_eig(genes_pca, ncp=30, addlabels=T, barfill="deepskyblue2", barcolor="deepskyblue4")
```

\  

First 4 PCs look enough to classify the types of cancer despite only accounting for 32.27% of the variability of the data, which suggests high collinearity:

```{r echo=FALSE, fig.width=3, fig.height=3}
genes_colours = rainbow(5, s=0.5)[cancer+1]

plot.new()
legend('center', bty = "n", c("BRCA","COAD","KIRC","LUAD","PRAD"), lty = 1, lwd = 2,
       col = c(rainbow(5, s=0.5)[1], rainbow(5, s=0.5)[2], rainbow(5, s=0.5)[3], rainbow(5, s=0.5)[4], rainbow(5, s=0.5)[5]))
```

```{r echo=FALSE}
pairs(genes_pca$x[,1:4], pch=19, col=genes_colours)
```






\newpage






## Fully connected network

\  

A fully connected network is trained with 80% of the data:

```{r include=FALSE}
library(keras)

training_index = sample(1:nrow(genes), round(nrow(genes) * 0.8))

x_train = as.matrix(genes[training_index,])
x_test = as.matrix(genes[-training_index,])

y_train = to_categorical(cancer[training_index])
y_test = to_categorical(cancer[-training_index])

black_box <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax")

black_box %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

validation_index = sample(1:nrow(x_train), nrow(x_train)*0.2)

black_box_history <- black_box %>% fit(
  x_train[-validation_index,],
  y_train[-validation_index,],
  epochs = 4,
  batch_size = 32,
  validation_data = list(x_train[validation_index,],  y_train[validation_index,])
)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(black_box_history)
```

\  

The accuracy of the network is close to 1 (sometimes 1 depending on the random initialization of the network):

```{r echo=FALSE, message=FALSE, warning=FALSE}
black_box_pred = black_box %>% predict(x_test)
black_box %>% evaluate(x_test, y_test)
```

\  

Neural networks use to be power prediction tools but they come with a cost in terms of interpretability of the predictions. They contain a huge number of parameters making difficult the identification of features that are influetial in the response of the model.









\newpage







## Surrogate model

\  

To tackle the interpretability issue of the prediction model (which we'll call _black box_ from now on), a surrogate interpretable model is fitted in parallel. This surrogate model will help us to undestand the relationship between the features and the response at a global level (i.e. for no particular prediction).  

Lasso will be the model of choice, as it performs feature selection efficiently for high-dimensional data and it's very easy to understand.  

Because the purpose of this model is interpretation, we'll add a parameter on top of lasso to select the number of features we want to interpret the black box model. We'll fit several models with different lambdas and pick the one with the desired number of features. This is done for each category. With a fixed number of features we'll be able to fairly compare the surrogate model with other models with the same number of features as we'll later with LIME.  

20 features are selected (about 0.05% of the total features).

```{r echo=TRUE, fig.width=4, fig.height=4}
library(glmnet)

# this function looks for a lasso model with n_features selected for each category
# (if possible, it will depend on the lambda grid)
get_surrogate_features <- function(surrogate, n_features){

  surrogate_coeffs = coef(surrogate)
  
  coeffs = vector(mode = "list", length = 5)
  names(coeffs) = c("0","1","2","3","4")
  index = coeffs
  lambda = coeffs
  
  for (i in 0:4) {
  
    # code I picked up from the LIME package: https://github.com/thomasp85/lime/blob/49df0a131deee4919a29bb6093c116b80b766d3d/R/lime.R#L22
    lasso_sparse = surrogate_coeffs[[as.character(i)]]
    has_value <- apply(lasso_sparse[-1,], 2, function(x) x != 0) 
    f_count <- apply(has_value, 2, sum)  # number of parameters for each lambda (columns in lasso_sparse)
    
    # In case that no model with correct n_feature size was found return features <= n_features
    lambda_index <- rev(which(f_count <= n_features))[1]
    
    # Selected features
    index[[as.character(i)]] = which(has_value[, lambda_index])
    coeffs[[as.character(i)]] = lasso_sparse[which(has_value[, lambda_index])+1,lambda_index]
    lambda[[as.character(i)]] = surrogate[["lambda"]][lambda_index]
    
    #TODO: this is from the lime package
    #fit <- glmnet(x_train[,index[[as.character(i)]]], cancer[training_index], alpha = 0, lambda = 2 / length(cancer[training_index]))
    #r2 <- fit$dev.ratio
    #fff <- coef(fit)[-1, 1]
    #print(fff)
    #coeffs[[as.character(i)]] = fff#fit$beta@x
    #names(coeffs[[as.character(i)]]) = names(index[[as.character(i)]])
  }
  
  list(index = index, coeffs = coeffs, lambda = lambda)
}

n_features = 10
surrogate = glmnet(x_train, cancer[training_index], alpha = 1, family = "multinomial", nlambda=300, lambda.min.ratio=0.001)
surrogate_features = get_surrogate_features(surrogate, n_features)

plot(surrogate, xvar="lambda", label = TRUE, xlim=c(-8,-1))
```

\  

The plots above show the coefficient values for the selected features for each category. The higher the absolute value, the higher the influence (globally) in the output.  

Because the number of features is fixed, the selected value for lambda is not optimal for the fitting. Not a problem since the accuracy of the Lasso model is close to 1 with the test data:  
TODO: que pasa si el accuracy es mucho peor que el accuracy del black box model?  

```{r echo=TRUE}
# Each category has a different lambda. Here we use the smallest one for all the categories.
min_lambda = min(as.numeric(surrogate_features$lambda))
surrogate_test = predict(surrogate, newx = x_test, s = min_lambda, type="response")
surrogate_pred = max.col(as.data.frame(surrogate_test))-1

mean(cancer[-training_index] == surrogate_pred)
```

\  

The mean output probability (using 0 for misclassifications):

```{r echo=TRUE}
label_output_prob = apply(surrogate_test[,,1],1,max) * as.integer(as.integer(cancer[-training_index]) == surrogate_pred)
mean(label_output_prob)
```

\newpage

Lasso coefficients heatmaps:

```{r echo=FALSE, fig.height=15, fig.width=6, message=FALSE, warning=FALSE}
sparse_coeffs = coef(surrogate, s = min_lambda)

sparse = as.matrix(sparse_coeffs[["0"]])
sparse = cbind(sparse,as.matrix(sparse_coeffs[["1"]]))
sparse = cbind(sparse,as.matrix(sparse_coeffs[["2"]]))
sparse = cbind(sparse,as.matrix(sparse_coeffs[["3"]]))
sparse = cbind(sparse,as.matrix(sparse_coeffs[["4"]]))
colnames(sparse) = c("BRCA","COAD","KIRC","LUAD","PRAD")

sparse = sparse[-1,] #removing intercept row
coeff_matrix = sparse[rowSums(sparse) != 0,] #removing rows with all surrogate_coeffs set to 0

col_breaks = c(seq(-0.34,-0.0001,length=100),  # for red
               seq(+0.0001,+0.34,length=100))  # for blue

library(unikn)
library(gplots)
my_palette <- c(colorRampPalette(c("red","white"))(n = 99), "white", colorRampPalette(c("white","blue"))(n = 99))
heatmap.2(coeff_matrix, col= my_palette, breaks=col_breaks, dendrogram="row", symkey=FALSE, key=FALSE)
```

```{r echo=FALSE, fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
for(i in 1:nrow(coeff_matrix)){
  for(j in 1:ncol(coeff_matrix)){
    coeff_matrix[i,j] <- ifelse(coeff_matrix[i,j] < 0, -1,
                                ifelse(coeff_matrix[i,j] > 0, 1, 0))
  }
}

# Melt de hat_betas_fd + factores
library(reshape)
to_plot <- coeff_matrix
colnames(to_plot)  <- colnames(sparse)
aux <- c()
for(j in 1:ncol(coeff_matrix)){ # "Truco ordenación"
  aux <- c(aux, which(to_plot[,j]!=0))
}
aux <- rownames(to_plot)[aux]
aux <- unique(aux)
to_plot2 <- melt(to_plot)
colnames(to_plot2) <- c("gene","category","value")
to_plot2$gene <- factor(to_plot2$gene, levels = aux)
to_plot2$category <- factor(to_plot2$category)
to_plot2$value <- factor(to_plot2$value, levels = c(-1, 0, 1))

# Heatmap
library(ggplot2)
ggplot(to_plot2, aes(x = category, y = gene, fill = value)) + 
  geom_tile() +
  scale_fill_manual(values = c("red", "white", "blue")) +
  theme(axis.text.y = element_text(color="black", size=8),
        axis.ticks.y.left = element_blank())
```

\  

The variables selected by lasso give a global sense of the more influential predictors in driving the model, however, if the data is complex, in some regions of the input space the variables explaning the classifications could be completely different to the ones selected in the surrogate model.  





\newpage 








## LIME

\  

**Intuition**  

TODO  

(image taken from https://github.com/marcotcr/lime)
```{r echo=FALSE, out.width = '80%', fig.align="center"}
knitr::include_graphics("pictures_1/lime.png")
```


\  

**Formulation**  

TODO  

\  

**Algorithm**  

* In order to fit a local linear model around the data point which prediction we want to explain, we need enough data in the surroundings of that data point. In order to achieve that, kernel densities estimations are computed for each variable, then new simulated data points are introduced among the original data points by sampling from the kdes, increasing this way the "density" of the data that was used in the training of the black box. This is specially important with sparse data like our's where _p >> n_, where data points are "far" away from each other making for a poor local model fit.  

The parameter to tune in this step is the number of simulated data points (**n_permutations**).  
The risk if the parameter is too low, is the high variance of the simulated data each the same prediction is interpreted. If each time the same data point is explained the simulated data is different the variance in the interpretation will be high making the interpretations inconsistent and untrustworthy.  

TODO: no veo inconveniente en incrementar este parametro al maximo hasta que las limitaciones en CPU/memoria lo permitan.  

TODO: Los estimated kernel densities de cada variable no tienen en cuenta las relaciones entre las variables lo cual puede generar data points "irreales". Idea -> aplicar dimensionality reduction y usar multivariate kernel density estimation para generar un data set mas real.  

\  

* Of data points simulated by sampling from the features kdes, we are specially interested in those in the surroundings to the data point of interest, since we are fitting a local model. So we need to give more weight to data near the instance of interest and this is done with a smoothing exponential kernel. The width of the kernel is a parameter of the LIME model (**kernel_width**) and probably the more tricky one as shown in the following example of a 1-dimensional dataset:  

(image taken from https://christophm.github.io/interpretable-ml-book/lime.html)
```{r echo=FALSE, out.width = '80%', fig.align="center"}
knitr::include_graphics("pictures_1/kernel.png")
```

In this example changes in the kernel width leads to drastic changes in the local linear fit for the instance of interest (the cross in the plot). Adding more dimensions would increase the sensitivity of the width parameter even more.  

In the _lime_ package the default value is $0.75 \sqrt{p}$. The more number of dimensions for the same number of observations the more space is the data space, that's why the kernel width is increased depending on $p$.  
The appropriate value seems to depend on the surroundings of the data point being explained so there isn't a clear rule of thumb to follow for this parameter.  

Too small values could lead to insufficient data to fit the local model and too much variance in different interpretations for the same dat point to explain.  
Too high values could lead to loosing "locality", hence the included data becoming less linear and the explanation of the local model less accurate.  

TODO: Buscar algun criterio, quizas basado en la complejidad (clasificaciones con menos o mas certidumbre) para seleccionar el kernel width.  

TODO: Leer _[2] In high-dimensional data, data points are sparse. Defining a ‘’local neighborhood’’ of the instance of interest may not be straightforward. Importance of the local neighbourhood is presented for example in the article ,,On the Robustness of Interpretability Methods’’ (Alvarez-Melis and Jaakkola 2018). Sometimes even slight changes in the neighbourhood affects strongly obtained explanations._

Another parameter is the distance function that measures the proximity between the instance of interest and the simulated data points. The default option is Gower’s distance but others like Euclidean or Manhattan (see _?dist()_ for details) can also be used.  

TODO: Elegir la funcion mas adecuada teniendo en cuenta:  
- la alta dimensionalidad  
- la alta colinearidad  
- variables no-normales (pero tampoco exponenciales)  
- las variables han sido standardizadas  
_[3] It is very unclear whether the distance measure should treat all features equally. Is a distance unit for feature x1 identical to one unit for feature x2? Distance measures are quite arbitrary and distances in different dimensions (aka features) might not be comparable at all._  

\  

* The next step is to feed the black box with the simulated data to get the classifications required to fit the local model.  

\  

* With the simulated data and the corresponding predictions, a linear model is fit. Several options are available (**feature_select**). We'll choose lasso for two reasons: the high-dimensionality of the data and to get a better comparison with the global surrogate lasso model. Lasso will use the weights from the smoothing kernel to give more influence to the the neighborhood to the original observation to be explained.  

\  

* Finally the coefficients with higher absolute values are selected (**n_features**) to explain the output (**n_labels**, _1_ if we are just interested in the selected category).  






\newpage






## LIME with the original variables

\  

**Picking instances to evaluate the interpretation model.**  

* picking samples that are not close together in the imput space in order to cover different scenarios.  

* picking samples with less certainty in the output - samples that lie within the frontier between different categories are tougher to predict and therefore more intersting to understand.  

* TODO: mirar el metodo propuesto en "4.SUBMODULAR PICK FOR EXPLAINING MODELS" en el paper [1]  

```{r include=FALSE}
library(lime)

# Setup of lime::model_type()
model_type.keras.engine.sequential.Sequential <- function(x, ...) {"classification"}

# Setup of lime::predict_model()
predict_model.keras.engine.sequential.Sequential <- function (x, newdata, type, ...) {
  pred <- predict(object = x, x = as.matrix(newdata))
  data.frame(BRCA = pred[,1], COAD = pred[,2], KIRC = pred[,3], LUAD = pred[,4], PRAD = pred[,5])
}
```


We'll pick up two observations to explain, one from category _PRAD_ which data points in the PCA plots look like they don't overlap too much with other categories (easy to predict), and another one from category _LUAD_ which data points overlap more with other categories. Each one will be interpreted 4 times to analyse the consistency of the selected features and their weights.  

As seen in the plots below, the results are very inconsistent - most of the 20 features are different for the same data point. The problem is probably coming from the high-dimensionality of the data, too many features are "competing" to explain the same variance. Depending on the simulated data created in the interpretation, some predictors will prevail over other predictors, even if the change in the simulated data is subtle.  

The low $R^2$ of the fits ("explanation fit" in the plot) highlights the issue.  

To try to work out this problem we could try to increase the number of simulated data points (default is 5000). However even with 10000 permutations (leading to a 10000*20000 matrix for lasso to fit the line, it takes ages) the problem persists. With more than 10000 permutations I get memory allocation errors.  

TODO: probar con un kernel width mas grande para reducir la varianza en las interpretaciones?  

TODO: idea -> en lugar de depender del smoothing kernel, filtrar del training data que se le pasa al explainer los data points alejados del punto de interes para asi obtener mas densidad alrededor del punto de interes con menos limitaciones en cuanto a CPU/memoria. seria esto distorsionar demasiado la interpretacion?  

TODO: sacar las interpretaciones de todos los data points de una misma categoria y comparar con lime?  

```{r echo=TRUE, fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
get_instance_explanation <- function(datapoints_index) {

  # the explainer buils the simulated data to fit the local model from the training data and the variable kdes
  explainer_all <- lime(
    x = as.data.frame(x_train),
    model = black_box,
    use_density = TRUE, # marginal kdes
    bin_continuous = FALSE
  )

  lime::explain(
    x = as.data.frame(x_test)[datapoints_index,],
    explainer = explainer_all, 
    n_permutations = 10000,
    #kernel_width = 0.75,
    feature_select = "lasso_path",
    n_features = n_features,
    n_labels = 1
  )
}

# PRAD category
#plot_features(get_instance_explanation(rep(which(cancer[-training_index] == 4)[1],4)))  # training_index was random

# LUAD category
#plot_features(get_instance_explanation(rep(which(cancer[-training_index] == 3)[1],4)))
```


TODO:
probar con distintos kernel widths (distintas combinaciones: mas o menos categoryoverlap, mas o menos varianza (pcs vs all individual genes))
probar con ruido en el modelo de clusters?
buscar literatura como sobre abordar p >> n
buscar data sets especificos para cada interpretable data representation (time series con segmentos, ...)
heatmap de los parametros con distintos data points para ver donde se parece mas lime a global lasso (o algo asin)




\newpage 






## LIME with discretized variables

\  

To reduce the variability of the the interpretations, an option is to discretize the data space. This is done in the _lime_ package with the parameter **bin_continuous** set to _true_ and specifying the number of bins for the variable values (quantiles are computed for each variable to segment the distribution in categories - there is no actual order).  

We try again the same samples as before. Some observations:  

For the PRAD category (doesn't overlap with other categories)  
* Only a few features are significant (between 1 and 4 in my tests).  
* The result is more consistent (more common features) than with continuous variables, but still there are some differences.  
* $R^2$ is high.  

For the LUAD category (overlaps with other categories)  
* Only one feature is significant.  
* The dominant feature is the same in all the cases - very consistent.  
* $R^2$ is very low.  

TODO: explicar por que ocurre lo anterior descrito.  

The interpretation with ranges of values is probably more clear. The discretization of the data in the interpretations will be acceptable depending on the domain, some practitioners using the black box as a tool might not need the level of "granularity" of continuous values and will prefer more "abstract" interpretations.

```{r echo=TRUE, fig.height=8, fig.width=8, message=FALSE, warning=FALSE}
get_discrete_instance_explanation <- function(datapoints_index, explainer) {

  explanation_all_discrete  = lime::explain(
    x = as.data.frame(x_test)[datapoints_index,],
    explainer = explainer, 
    n_permutations = 10000,
    #kernel_width = 0.75,
    feature_select = "lasso_path",
    n_features = n_features,
    n_labels = 1
  )
}

explainer_all_discrete = lime(
  x = as.data.frame(x_train),
  model = black_box,
  #use_density = TRUE,
  bin_continuous = TRUE,
  n_bins = 10
)

# PRAD category
#plot_features(get_discrete_instance_explanation(rep(which(cancer[-training_index] == 4)[1],4),explainer_all_discrete))

# LUAD category
#plot_features(get_discrete_instance_explanation(rep(which(cancer[-training_index] == 3)[1],4),explainer_all_discrete))
```

\  

TODO: relacion con el modelo lasso global (por el momento me esta dando un error de memoria al crear el modelo lasso global con variables categoricas:)

```{r eval=FALSE, include=FALSE}
x_train_discrete = genes[training_index,]

for (i in 1:ncol(x_train_discrete)) {
  
  discrete_values = as.factor(cut(genes[training_index,i], unique(explainer_all_discrete$bin_cuts[[i]]), include.lowest = TRUE))

  # Lasso needs at least 2 levels for categorical values
  if (length(levels(discrete_values)) > 1)
    x_train_discrete[,i] = as.vector(discrete_values)
  else
    x_train_discrete[,i] = NA
}

x_train_discrete = t(na.omit(t(x_train_discrete)))
x_train_discrete_m = model.matrix( ~ ., as.data.frame(x_train_discrete))
```






\newpage






## LIME with selected predictors

TODO: Remover predictores que sean colineares. mctest es demasiado lento para tantas dimensiones, incluso con solo 1000 variables le llevaría horas. He probado con forward selection (ver condigo abajo) pero sigue siendo demasiado lento para cubrir todas las variables.  
TODO: usar elastic net?  
TODO: calcular cuanto se pierde en la prediccion en el modelo lasso  

```{r echo=TRUE}
library(mctest)

selected = c(colnames(genes)[1])

max = ncol(genes)

if (FALSE)
for (i in 2:max) {
  print(paste(i,"..."))
  selected = c(selected, colnames(genes)[i])
  result = as.data.frame(mctest(genes[,selected], cancer, type="i", method="VIF")[["idiags"]])
  selected = rownames(result[result$detection == 0,])
  print(selected)
}
```





\newpage






## Interpretable data representation (interpretable variable space)

As we have seen with the data space discretization example, the data representation for the interpretations can be different from the data representation used for in the predictions. We could use whatever is more convenient for the interpretation, as long as we keep a mapping between both data representations.  

A classic example of this are superpixels from images in image classification. A superpixel represents a segment of an image that group pixeles that are interconnected and share similar colors. As opposed to individual pixels, this representation is natural for humans and simplifies the identification of specific regions that could have high influence in the classification of the image. For instance, if a machine learning model classifies the below picture as a goose, it's very likely that the superpixel representing the beak was selected in the LIME interpretation. The simulated data in this case is represented by copies of the original picture where some superpixels are zeroed (set to white), so the local model is able to distinguish between superpixels that have an impact in the classification and those that are less relevant.  

(image taken from https://pbiecek.github.io/ema/LIME.html)
```{r echo=FALSE, out.extra='', out.width = '100%', fig.align="center"}
knitr::include_graphics("pictures_1/goose_poodle.png")
```





\newpage





## Interpretation with clusters of variables

Taking the superpixel example as inspiration we could look for a more abstract data representation easier to undestand that individual variables in our tabular data. In a picture, pixels are correlated by color similarity and by proximity in the spacial axes. In tabular data, variables could be grouped by linear correlation. The interpretable data representation would be then clusters grouping the original variables. An expert in the domain of our prediction model would be able to advise if the representation is useful.  

Clustering could be based on:  
- correlation of variables regardless the classification  
- multicollinearity using package mctest  
- some grouping based on PCA weights  
- ...  

We'll create 100 clusters based on correlation:  

TODO: mirar "grouped lasso" para comparar con las interpretaciones de grupos de genes muy correlacionados.  
TODO: hacerlo con con multicolinearidad  

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(ClustOfVar)
#TODO: dig more into this package (hclustvar, ...)
#TODO: other libraries: corclust

num_clusters = 100
kmeansvar = kmeansvar(X.quanti = as.matrix(genes), X.quali = as.matrix(as.factor(cancer)), init = num_clusters, iter.max = 5, matsim = FALSE) # nstart = 1, 

clusters = kmeansvar$cluster
clusters = clusters[1:(length(clusters)-1)]  #X.quali column?
```

\  

80% of the clusters will be randomly zeroed in each simulated data point in order for the model to find relevant clusters for the classification of the instance of interest.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
# "p is the percentage of non-zeroed clusters to use in each simulated data point
get_cluster_explanation <- function(datapoint_index, p = 0.2)
{
  # the instance of interest to interpret
  instance = as.data.frame(x_test)[datapoint_index,]

  preprocessing <- function(x) {

    # the instance of interest is replicated n_permutations times
    toblackbox = instance[rep(1, nrow(x)),]

    # then, 0s are set for all the variables contained in an inactive cluster
    # (the permutations will randomly active p% of the clusters of each simulated data point)
    for (i in 1:nrow(x)) {
  
      for (k in 1:ncol(x))
      {
        if (!x[i,k])
          toblackbox[i,names(clusters[clusters == k])] = 0
        
        # alternatively, variable means instead of 0s (doesn't change the result too much):
        #vars_in_cluster = names(clusters[clusters == k])
        #  
        #if (length(vars_in_cluster) > 1)
        #  toblackbox[i,names(clusters[clusters == k])] = apply(x_train[,vars_in_cluster], 2, mean)
        #else
        #  toblackbox[i,names(clusters[clusters == k])] = mean(x_train[,vars_in_cluster])
      }
    }

    as.matrix(toblackbox)
  }

  # 
  sim_dist = as.data.frame(matrix(FALSE, nrow = 100, ncol = num_clusters))
  sim_dist[1:round(p*100),] = TRUE
  
  for (i in 1:num_clusters)
    colnames(sim_dist)[i] = as.character(sprintf("cluster_%s",i))
  
  explainer_cluster <- lime(
    x = as.data.frame(sim_dist),
    model = black_box,
    use_density = TRUE,
    preprocess = preprocessing,
  )
  
  # vector of active clusters (all 1s for the instance of interest)
  all_ones = as.data.frame(matrix(TRUE, nrow = 1, ncol = num_clusters))
  for (i in 1:num_clusters)
    colnames(all_ones)[i] = as.character(sprintf("cluster_%s",i))
  
  lime::explain(
    x = all_ones,
    explainer = explainer_cluster, 
    n_permutations = 500,
    #kernel_width = 0.75,
    feature_select = "lasso_path",
    n_features = n_features,
    n_labels = 1
  )
}

# PRAD category
plot_features(get_cluster_explanation(which(cancer[-training_index] == 4)[1]))

# LUAD category
plot_features(get_cluster_explanation(which(cancer[-training_index] == 3)[1]))
```







\newpage






## Interpretation with PCA

Another way to get a more interpetable data representation would be through dimension reduction like PCA. Again, the expert in the domain has to advise if the representation would be useful for interpretation. The expert might be able to understand the meaning of the main components.  

For a fair comparison with the global surrogate lasso model, we'll do interpretations with lime of observations in the training data.  
The number of components in the interpretable data representation could be reduced to explain a percentage of the explained variance, since probably only a few dozens of them will be really influencial. We'll start with the first 100 PCs for now.  

```{r echo=FALSE, fig.align="center", fig.height=4, fig.width=6}
x_train_pca = prcomp(x_train)

genes_eigenvalues = get_eigenvalue(x_train_pca)
plot(1:nrow(x_train), genes_eigenvalues$cumulative.variance.percent, xlab = "number of PCs", ylab = "cumulative variance %", type = "l")
```

\  

The black box only understands data as represented originally with individual gene expressions (the optimal representation for prediction rather than PCA which destroys information), therefore the simulated data in PCA format has to be converted back to the original format in order to get predictions (see _preprocessing_ function below).  

```{r echo=TRUE, fig.height=8, fig.width=8, warning=FALSE}
# The number of pcs to use in the interpretable data representation:
pcs_n = 100

get_PCA_explanation <- function(datapoints_index, n_permutations = 2000, kernel_width = 0.75)
{
  preprocessing <- function(x){
  
    # reversing PCA (with the remaining info) to feed the black box which only understands individual genes
    a = as.matrix(x) %*% t(x_train_pca$rotation[,1:pcs_n])
    b = t(a) + x_train_pca$center
    t(b)
  }
  
  explainer_PCA <- lime(
    x = as.data.frame(x_train_pca$x)[,1:pcs_n],
    model = black_box,
    use_density = TRUE,
    preprocess = preprocessing,
    bin_continuous = FALSE
  )

  lime::explain(
    # converting test data point coordinates to the PCA space:
    x = as.data.frame(x_train[datapoints_index,] %*% x_train_pca$rotation[,1:pcs_n]),
    explainer = explainer_PCA, 
    n_permutations = n_permutations,
    kernel_width = kernel_width,
    feature_select = "lasso_path",
    n_features = 10,
    n_labels = 1
  )
}
```

```{r eval=FALSE, include=FALSE}
# proving adding more data points doesn't impact the original data points in the PCA reversal
# only the information lost in the compression is missing after the PCA reversal
extended = genes_pca$x
for (i in 1:500) extended = rbind(extended,rep(1,nrow(genes)))

reverse = t(t(extended %*% t(genes_pca$rotation)) + genes_pca$center)
reverse_pca = prcomp(reverse)

par(mfrow=c(1,2))
plot(genes_pca$x[,7:8], pch=19, col=genes_colours)
plot(reverse_pca$x[,7:8], pch=19, col=genes_colours)
```

\  

We analyse the interpretation of an observation of category PRAD (this category barely overlaps with other categories), and we try the interpretation of another observation of category LUAD (this category overlaps with other categories as seen in the PCA plots).  
For both observations the process is run 4 times to check the consistency of the interpretation. The 10 more influential components are displayed.  

The results are:  

- For PRAD (doesn't overlap with other categories) the results are consistent. $R^2$ is usually high (depends on the randomly selected observation). The interpretation is dominated by 1 or 2 components.  

- For LUAD (overlaps with other categories) the selected components are also rather consistent. $R^2$ is also usually high. The interpretation is also dominated by 1 or 2 components but they are not as dominant as with category PRAD and the non-dominant features are more repeated between interpretations.  

```{r echo=TRUE, fig.width=8, fig.height=8}
# PRAD category
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 4)[1],4),5000))

# LUAD category
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000))
```

```{r eval=FALSE, fig.height=8, fig.width=8, include=FALSE}
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000,0.25))
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000,0.50))
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000,0.75))
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000,1.25))
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000,1.75))
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000,3))
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000,5))
plot_features(get_PCA_explanation(rep(which(cancer[training_index] == 3)[1],4),5000,10))
```

\  

We now check the correlation between the PCA version of the global surrogate model and the local lime models.  

The lasso surrogate model is fitted again with 10 selected components to fairly compare them with the 10 selected features with lime (as close to 10 features as possible, it depends on the lambda grid and the category):  

```{r message=FALSE, warning=FALSE, include=FALSE}
gc()
```

```{r echo=FALSE}
surrogate_pca = glmnet(x_train_pca$x[,1:pcs_n], cancer[training_index], alpha = 1, family = "multinomial", nlambda=300, lambda.min.ratio=0.00001)
surrogate_pca_features = get_surrogate_features(surrogate_pca, 10)

coeffs_pca_sur = setNames(data.frame(matrix(ncol = pcs_n, nrow = 0)), as.vector(sprintf("PC%i",1:pcs_n)))

for (i in 0:4){

  lop = surrogate_pca_features[["coeffs"]][[as.character(i)]]

  for (j in 1:(length(lop)))
    coeffs_pca_sur[i+1,names(lop)[j]] = lop[j]
}

rownames(coeffs_pca_sur) = paste(levels(labels),"(global)")
coeffs_pca_sur[is.na(coeffs_pca_sur)] = 0

print(coeffs_pca_sur[,colSums(coeffs_pca_sur) != 0])
```

\  

The lime local models are fitted for all the observations in categories LUAD and PRAD (more than 100 each) and using 3000 permutations for each interpretation.  

```{r message=FALSE, warning=FALSE, include=FALSE}
get_category_coeffs <- function(category)
{
  datapoints_index = which(cancer[training_index] == category)
  
  explanation_PCA = get_PCA_explanation(datapoints_index[1],3000)
  
  for (i in 2:length(datapoints_index))
  {
    temp = get_PCA_explanation(datapoints_index[i],3000)
    temp$case = i
    explanation_PCA = rbind(explanation_PCA, temp)
  }

  explanation_PCA_cases = tidyr::nest(explanation_PCA, key = -case)
  
  coeffs_cat = setNames(data.frame(matrix(ncol = pcs_n, nrow = 0)), as.vector(sprintf("PC%i",1:pcs_n)))
  coeffs_names = vector()
  
  for (i in 1:nrow(explanation_PCA_cases)) {
    
    c = explanation_PCA_cases[[2]][[i]]
    
    for (j in 1:length(c$feature))
      coeffs_cat[i,c$feature[j]] = c$feature_weight[j]
    
    coeffs_names = c(coeffs_names, paste(levels(labels)[category+1],explanation_PCA_cases$case[i]))
  }
  
  rownames(coeffs_cat) = coeffs_names
  coeffs_cat[is.na(coeffs_cat)] = 0
  
  coeffs_cat
}

coeffs_LUAD = get_category_coeffs(3)
coeffs_PRAD = get_category_coeffs(4)
```

```{r echo=FALSE}
print(coeffs_LUAD[1:10,colSums(coeffs_LUAD) != 0])
print(coeffs_PRAD[1:10,colSums(coeffs_PRAD) != 0])
```

\  

In order to compare the global lasso model with the lime models, the coefficients of the selected features in the interpretations of all the observations for the same category are summarized in a single list of component coefficients. Each summarized component coefficient will be computed with a statistic describing the central tendency of the coefficient across the observations.  

To decide which statistic to use we analyse the more influential features for each category. To measure the influence we use the sum of the absolute values of the coefficients of all the observations for each component:  

```{r echo=FALSE}
abs_sums_PRAD = apply(abs(coeffs_PRAD), 2, sum)
infl_comp_PRAD = sort(abs_sums_PRAD, decreasing = TRUE)[1:16]

abs_sums_LUAD = apply(abs(coeffs_LUAD), 2, sum)
infl_comp_LUAD = sort(abs_sums_LUAD, decreasing = TRUE)[1:16]

cat("\nsum of the absoulte values of the coefficients for PRAD:\n")
round(infl_comp_PRAD,5)

cat("\nsum of the absoulte values of the coefficients for LUAD:\n")
round(infl_comp_LUAD,5)
```

\  

Below are displayed the distributions of the 16 more influential components for both categories. The vertical red line represents the mean, the blue one represents the median.  

```{r echo=FALSE, fig.align="center", fig.height=6, fig.width=10, message=FALSE, warning=FALSE}
library(tidyr)
library(purrr)
library(ggplot2)

means_PRAD = apply(coeffs_PRAD, 2, mean)
medians_PRAD = apply(coeffs_PRAD, 2, median)
key_PRAD = names(infl_comp_PRAD)[1:16]

coeffs_PRAD[,key_PRAD] %>%
  keep(is.numeric) %>% 
  gather(key_PRAD) %>% 
  ggplot(aes(value)) +
  facet_wrap(~ factor(key_PRAD, levels=names(infl_comp_PRAD)[1:16]), scales = "free") + 
  geom_density() + 
  geom_rug(colour="red") +
  geom_vline(xintercept=means_PRAD[key_PRAD], size=0.5, color="red") + 
  geom_vline(xintercept=medians_PRAD[key_PRAD], size=0.5, color="blue") + 
  ggtitle("16 more influential components for PRAD classification") + 
  theme(plot.title = element_text(hjust = 0.5))


means_LUAD = apply(coeffs_LUAD, 2, mean)
medians_LUAD = apply(coeffs_LUAD, 2, median)
key_LUAD = names(infl_comp_LUAD)[1:16]

coeffs_LUAD[,key_LUAD] %>%
  keep(is.numeric) %>% 
  gather(key_LUAD) %>% 
  ggplot(aes(value)) +
  facet_wrap(~ factor(key_LUAD, levels=names(infl_comp_LUAD)[1:16]), scales = "free") + 
  geom_density() + 
  geom_rug(colour="red") + 
  geom_vline(xintercept=means_LUAD[key_LUAD], size=0.5, color="red") + 
  geom_vline(xintercept=medians_LUAD[key_LUAD], size=0.5, color="blue") + 
  ggtitle("16 more influential components for LUAD classification") + 
  theme(plot.title = element_text(hjust = 0.5))
```

\  

We can see that the more influential components are more or less symmetric, whereas the less influentual ones are bimodal, one of the modes lying on value 0 which represents the absence of influence for a subset of observations.  

To avoid the components that only appear as influential for a few observations (and when they are included in the list of 10 more important components their value is very small) we use the median to compute the representing value of the component coefficient for the whole category. The median will ignore components that come up rarely and have small values by setting them to 0 in the summarize component coefficient.  

Also note that the variance of the coefficients for the more dominant components is higher in category LUAD (the one overlapping other categories in the data space - harder to predict):  

```{r echo=FALSE}
cat("\nstandard deviation of first 3 more influential component coefficients for PRAD:\n")
round(apply(coeffs_PRAD[,key_PRAD[1:3]], 2, sd),5)

cat("\n\nstandard deviation of first 3 more influential component coefficients for LUAD:\n")
round(apply(coeffs_LUAD[,key_LUAD[1:3]], 2, sd),5)
```

\  

We compute a single set of coefficients for each category using the median of the components coefficients from all the observations for both categories, and get a histogram to compare them with the surrogate lasso model coefficients:  

```{r echo=FALSE, fig.height=9, fig.width=6, message=FALSE, warning=FALSE}
fun = median

coeffs_pca = t(as.matrix(coeffs_pca_sur))
coeffs_pca = cbind(coeffs_pca,apply(coeffs_LUAD, 2, fun))
coeffs_pca = cbind(coeffs_pca,apply(coeffs_PRAD, 2, fun))

coeffs_pca = coeffs_pca[,c(4,6,5,7)]
colnames(coeffs_pca) = c("LUAD  (global)", "LUAD (lime)", "PRAD (global)", "PRAD (lime)")

# removing rows with all 0s
coeffs_pca = coeffs_pca[rowSums(coeffs_pca) != 0,]

# normalizing
library(wordspace)
coeffs_pca = normalize.cols(coeffs_pca)

max_c = max(abs(coeffs_pca))
col_breaks = c(seq(-max_c,-0.0001,length=100),  # for red
               seq(+0.0001,+max_c,length=100))  # for blue

library(unikn)
library(gplots)
my_palette <- c(colorRampPalette(c("red","white"))(n = 99), "white", colorRampPalette(c("white","blue"))(n = 99))
heatmap.2(coeffs_pca, col= my_palette, breaks=col_breaks, symkey=FALSE, key=TRUE, Rowv=NA, Colv=NA,
          offsetCol = -53, srtCol=45, cexCol=1.2, adjCol = c(0,1))
```

We can see there is high correlation, specially for category LUAD.  

TODO: por que LUAD se ajusta mejor?  
TODO: por que PC1 y PC2 son tomados en cuenta en lime pero no el global lasso?  
TODO: he tenido que normalizar las filas, por que los coeficientes en lime son un orden de magnitud mas pequenos comparados con los coeficientes de global lasso. Por que esa diferencia? Coeficientes originales:  
```{r echo=FALSE}
coeffs_pca = t(as.matrix(coeffs_pca_sur))
coeffs_pca = cbind(coeffs_pca,apply(coeffs_LUAD, 2, fun))
coeffs_pca = cbind(coeffs_pca,apply(coeffs_PRAD, 2, fun))
colnames(coeffs_pca) = c(colnames(coeffs_pca)[1:5], "LUAD (lime)", "PRAD (lime)")
print(coeffs_pca[1:10,])
```


\  

Correlation plot of coefficients between lime interpretations of 20 observations and the global lasso model for the same category:  

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(corrplot)
cof = coeffs_pca_sur[4,]
cof = rbind(cof, coeffs_LUAD[1:20,])
cof = normalize.rows(as.matrix(cof))

models_c = t(cof)
corrplot(cor(models_c),is.corr=T)


cof2 = coeffs_pca_sur[5,]
cof2 = rbind(cof2, coeffs_PRAD[1:20,])
cof2 = normalize.rows(as.matrix(cof2))

models_c2 = t(cof2)
corrplot(cor(models_c2),is.corr=T)
```

We can see that there is less correlation between coefficients for category PRAD (easier to predict). The reason for this could be that with PRAD we get 1 or 2 dominant components explaining the predictions, while the rest of the components are less relevant and therefore have smaller and more variable values. With LUAD, different observations have more components explaining the prediction in common, even if the variance of the weights of these common components is higher compared to PRAD.




```{r eval=FALSE, include=FALSE}
- returned object (original data)
case - rowname of the data point
data - data point values
feature_value
prediction - original prediction from the black box model

- returned object (prediction model)
prediction - original prediction from the black box model
label_prob
label

- returned object (explanation model)
model_r2
model_intercept
model_prediction
feature
feature_weight
how about the predictions of the explanation model? could it be a difference between the prediction and the explanation model for the selected category? Creo que en realidad eso no tiene mucha importancia.

Analizar:
buscar similitudes entre LASSO y LIME, some plot showing correlation between LIME and LASSO
- Creo que para ser justos deberíamos utilizar LIME con todas las observaciones de entrenamiento para predecir sólo la clase que nos da el modelo de entrenamiento, y con un número de features igual al número de betas diferentes de 0 que tenemos en el LASSO para esa clase (TAREA 4)
elastic net (0.5)?

intercetps? important? meaning?

TODO: dibujar en plot PC1 vs PC2 vs las interpretaciones intereantes (limites, etx)

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

**Is the explanation good?**

The resulting model can then be used to explain the predictions of the more complex model at the locality of the observation of interest.
If R^2 is poor we shouldn’t put too much faith in the explanation.
If R^2 is poor but with little simulated data, should we still put faith?

The expert will give feedback to se if the result make sense and we can trust the black model. What if we need to change the LIME parameters?

The fidelity measure (how well the interpretable model approximates the black box predictions) gives us a good idea of how reliable the interpretable model is in explaining the black box predictions in the neighborhood of the data instance of interest. (is this r^2?)

The correct definition of the neighborhood is a very big, unsolved problem when using LIME with tabular data. In my opinion it is the biggest problem with LIME and the reason why I would recommend to use LIME only with great care. For each application you have to try different kernel settings and see for yourself if the explanations make sense. Unfortunately, this is the best advice I can give to find good kernel widths. (no hay error que minimizar)

The complexity of the explanation model (like the number of explanatory variables?) has to be defined in advance. This is just a small complaint, because in the end the user always has to define the compromise between fidelity and sparsity.

Another really big problem is the instability of the explanations. In an article 38 the authors showed that the explanations of two very close points varied greatly in a simulated setting. Also, in my experience, if you repeat the sampling process, then the explantions that come out can be different. Instability means that it is difficult to trust the explanations, and you should be very critical.

_[2] Our choice of G (glass-box sparse linear models) means that if the underlying model is highly non-linear even in the locality of the prediction, there may not be a faithful explanation.  However, we can estimate the faithfulness of the explanation on Z, and present this information to the user. This estimate of faithfulness can also be used for selecting an appropriate family of explanations from a set of multiple interpretable model classes, thus adapting to the given dataset and the classifier. We leave such exploration for future work, as linear explanations work quite well for multiple black-box models in our experiments._

Considering there are several knobs we can adjust when performing LIME, we can treat these as tuning parameters to try to tune the local model. This helps to maximize the amount of trust we can have in the local region explanation. As an example, the following code block changes the distance function to be Euclidean, increases the kernel width to create a larger local region, and changes the feature selection approach to a LARS-based LASSO model.
The result is a fairly substantial increase in our explanation fits, giving us much more confidence in their explanations.

## References

[1] Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. “Why Should I Trust You?: Explaining the Predictions of Any Classifier.” In Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, 1135–44. ACM.

[2] Przemyslaw Biecek and Tomasz Burzykowski. 2020. “Explanatory Model Analysis: Explore, Explain and Examine Predictive Models.” E-Book At< https://pbiecek.github.io/ema/>.

[3] Molnar, Christoph, and others. 2018. “Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.” E-Book At< Https://Christophm.github.io/Interpretable-Ml-Book/>.

LIME Python code:  
https://github.com/marcotcr/lime

Port in R:  
https://github.com/thomasp85/lime

```





