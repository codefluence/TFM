---
title: "gene expression cancer RNA-Seq"
date: "20/02/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data description

Source: Samuele Fiorini, samuele.fiorini@dibris.unige.it, University of Genoa, redistributed under Creative Commons license.  
Download: https://www.kaggle.com/murats/gene-expression-cancer-rnaseq

\  

Number of observations: 801  
Number of predictors: 20532  

\  

The observations represent patients with primary tumors occurring in different parts of the body, covering 12 tumor types (the response categorical variable) including:  

* lung adenocarcinoma (LUAD)  
* breast carcinoma (BRCA)  
* kidney renal clear-cell carcinoma (KIRC)  
* colon adenocarcinoma (COAD)  
* prostate adenocarcinoma (PRAD)  

\  

All the predictors are continuous variables representing RNA-Seq gene expression levels measured by a sequencing platform.  

```{r include=FALSE}
data = read.csv(file = 'D:/books/data science/Machine Learning/TFM/data/data.csv')
labels = read.csv(file = 'D:/books/data science/Machine Learning/TFM/data/labels.csv')[,2]

genes = data[,-1] #removing sample name column

# uncomment to reduce the number of predictors
#genes = genes[,sample(1:ncol(genes), 5000)]

# a few columns only contain 0 values, these are removed
genes = genes[,colSums(genes != 0) > 0]
```



\newpage  



kde of 12 predictors picked at random. They look normal for the most part:  

```{r echo=FALSE}
par(mfrow = c(3,4))
for (i in sample(1:ncol(genes), 12)) {
  plot(density(genes[,i]))
}
```

\  

Response absolute frequencies:  

```{r echo=FALSE, fig.width=5, fig.height=3}
barplot(table(labels), col=rainbow(15, s=0.5), ylab="Absolute frequencies", xlab="Cancer Type")
```

\newpage 

55% of the variability in the data is explained by 10 PCs:  

```{r echo=FALSE}
library(factoextra)
genes_pca = prcomp(genes)
ev = get_eigenvalue(genes_pca)
fviz_eig(genes_pca,ncp=30,addlabels=T,barfill="deepskyblue2",barcolor="deepskyblue4")
```

\newpage 

First 2 PCs look enough to classify the types of cancer:  

```{r echo=FALSE}
cs = rep(1,nrow(genes))
cs = ifelse(labels=="COAD",2,cs)
cs = ifelse(labels=="BRCA",3,cs)
cs = ifelse(labels=="PRAD",4,cs)
cs = ifelse(labels=="KIRC",5,cs)
cs = ifelse(labels=="LUAD",6,cs)

colours = rainbow(7, s=0.5)[cs]

plot(genes_pca$x[,1:2], pch=19, col=colours)

legend('bottomright', bty = "n", c("COAD","BRCA","PRAD","KIRC","LUAD"), lty = 1, lwd = 2, col = c(rainbow(7, s=0.5)[2], rainbow(7, s=0.5)[3], rainbow(7, s=0.5)[4], rainbow(7, s=0.5)[5], rainbow(7, s=0.5)[6]))
```



\newpage 



## Lasso

Since p >> n, the estimation of the coefficients in a linear model will suffer from high variance. A lasso model is fit in order to reduce the output error (introducing some bias but largely decreasing variance). Lasso also performs variable selection which helps with interpretability. The aim is to reduce the number of predictors from more than 20,000 to just a bunch.  

80% observations are used to fit the model, the rest to measure the accuracy.  

```{r echo=TRUE, fig.width=5, fig.height=6}
training_index = sample(1:nrow(genes), round(nrow(genes) * 0.8))

x_lasso_train = as.matrix(genes[training_index,])
y_lasso_train = as.numeric(labels[training_index])-1  # categories start from 0 (as expected by keras)
x_lasso_test = as.matrix(genes[-training_index,])
y_lasso_test = as.numeric(labels[-training_index])-1  # categories start from 0 (as expected by keras)
```

```{r echo=TRUE, fig.width=6, fig.height=9}
# no need to scale the data, glmnet does it by default
library(glmnet)
cvfit = cv.glmnet(x_lasso_train, y_lasso_train, alpha = 1, family = "multinomial")
coeffs = coef(cvfit, s = "lambda.min")

par(mfrow = c(3,2))
plot(cvfit)

fit = glmnet(x_lasso_train, y_lasso_train, alpha = 1, family = "multinomial")
plot(fit, xvar="lambda", label = TRUE, xlim=c(-6,-1))

#TODO: find a way to apply the text to all the 5 plots returned by plot.glmnet
text(log(cvfit$lambda.min), coeffs[["4"]]@x[-1], labels=colnames(x_lasso_test[,coeffs[["4"]]@i[-1]]), pos=4, col="firebrick2", cex=0.7)
abline(v = log(cvfit$lambda.min))
```

# 0 BRCA
# 1 COAD
# 2 KIRC
# 3 LUAD
# 4 PRAD

The plots show the coefficient values for the selected predictors for each category. The higher the absolute value, the higher the influence (globally) in the output.  
Note that the numbers in the plot correspond to column indices. In the last plot, for category 4 (PRAD), the name of the predictors are displayed in red too.  


\  


The accuracy of the test data prediction is close to 1. The categories barely overlap in the input space as seen in the PC1 vs PC2 plot which only accounts for 25% of the variability of the data, leading to the high accuracy.

```{r echo=TRUE}
test = predict(cvfit, newx = x_lasso_test, s = cvfit$lambda.min, type="response")
pred = max.col(as.data.frame(test))-1

mean(y_lasso_test == pred)
```

The mean output (using 0 for misclassifications):

```{r echo=TRUE}
label_output_prob = apply(test[,,1],1,max) * as.integer(as.integer(y_lasso_test) == pred)
mean(label_output_prob)
```

\  

Lasso shrinks less significant coefficients to 0 as $\lambda$ increases (as in a linear optimization problem with constraint vertices in the predictor axes). With the optimal $\lambda$ computed numerically.  

The remaining significant predictors are:

```{r echo=TRUE}
sig_index = Reduce(union,c(coeffs[["0"]]@i,coeffs[["1"]]@i,coeffs[["2"]]@i,coeffs[["3"]]@i,coeffs[["4"]]@i))
# coeffs is a list of dgCMatrix
# @i are indices of non-zero values in the matrix (first one corresponds to the y-intercept)
# @x are the coefficients corresponding to the indices

colnames(x_lasso_train[,sig_index])
```

\  

Lasso coefficients heatmaps:

```{r echo=TRUE, fig.width=6, fig.height=15}
sparse = as.matrix(coeffs[["0"]])
sparse = cbind(sparse,as.matrix(coeffs[["1"]]))
sparse = cbind(sparse,as.matrix(coeffs[["2"]]))
sparse = cbind(sparse,as.matrix(coeffs[["3"]]))
sparse = cbind(sparse,as.matrix(coeffs[["4"]]))
colnames(sparse) = c("BRCA","COAD","KIRC","LUAD","PRAD")

sparse = sparse[-1,] #removing intercept row
coeff_matrix = sparse[rowSums(sparse) != 0,] #removing rows with all coeffs set to 0

col_breaks = c(seq(-0.34,-0.0001,length=100),  # for red
               seq(+0.0001,+0.34,length=100))  # for blue

library(unikn)
library(gplots)
my_palette <- c(colorRampPalette(c("red","white"))(n = 99), "white", colorRampPalette(c("white","blue"))(n = 99))
heatmap.2(coeff_matrix, col= my_palette, breaks=col_breaks, dendrogram="row", symkey=FALSE, key=FALSE)
```

```{r echo=TRUE, fig.width=8, fig.height=8}
for(i in 1:nrow(coeff_matrix)){
  for(j in 1:ncol(coeff_matrix)){
    coeff_matrix[i,j] <- ifelse(coeff_matrix[i,j] < 0, -1,
                                ifelse(coeff_matrix[i,j] > 0, 1, 0))
  }
}

# Melt de hat_betas_fd + factores
library(reshape)
to_plot <- coeff_matrix
colnames(to_plot)  <- colnames(sparse)
aux <- c()
for(j in 1:ncol(coeff_matrix)){ # "Truco ordenaciÃ³n"
  aux <- c(aux, which(to_plot[,j]!=0))
}
aux <- rownames(to_plot)[aux]
aux <- unique(aux)
to_plot2 <- melt(to_plot)
colnames(to_plot2) <- c("gene","category","value")
to_plot2$gene <- factor(to_plot2$gene, levels = aux)
to_plot2$category <- factor(to_plot2$category)
to_plot2$value <- factor(to_plot2$value, levels = c(-1, 0, 1))

# Heatmap
library(ggplot2)
ggplot(to_plot2, aes(x = category, y = gene, fill = value)) + 
  geom_tile() +
  scale_fill_manual(values = c("red", "white", "blue")) +
  theme(axis.text.y = element_text(color="black", size=8),
        axis.ticks.y.left = element_blank())
```

\newpage


## Densely connected network

The problem of classifying patients into types of cancer from a large number of predictors is similar to the classic example of classifying short newswires into topics (reuters 1986 dataset). Both are multiclass classifications from a very large number of predictors (in the reuters example, each predictor represents the presence or absence of a particular word - tens of thousands of usual words are included).  

There is no need to preprocess the data, the rows in our dataset are ready to fed the model as input vectors.  

```{r echo=TRUE}
library(keras)

# This scaling is advised...
mean = apply(genes[training_index,], 2, mean)
sd = apply(genes[training_index,], 2, sd)
x_train = as.matrix(scale(genes[training_index,], center = mean, scale = sd))
x_test = as.matrix(scale(genes[-training_index,], center = mean, scale = sd))
# ...but this way the model diverges (loss=NA in the first iteration) 
# TODO: why? #https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons

# scaling the union of training data and test data instead..
x_train = as.matrix(scale(genes)[training_index,])
x_test = as.matrix(scale(genes)[-training_index,])

y_train = to_categorical(as.numeric(labels[training_index])-1)
y_test = to_categorical(as.numeric(labels[-training_index])-1)
```

The model is trained in a matter of few seconds and yields an accuracy very close to 100%. No need for regularization or any other technique to help with the reduction of overfitting.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(keras)

gmodel <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>% 
  layer_dense(units = 64, activation = "relu") %>% 
  layer_dense(units = 5, activation = "softmax")

#gmodel

set.seed(123)
gmodel %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)

validation_index = sample(1:nrow(x_train), nrow(x_train)*0.2)

ghistory <- gmodel %>% fit(
  x_train[-validation_index,],
  y_train[-validation_index,],
  epochs = 8,
  batch_size = 32,
  validation_data = list(x_train[validation_index,],  y_train[validation_index,])
)

#print(ghistory)

(results <- gmodel %>% evaluate(x_test, y_test))

plot(ghistory)

output = gmodel %>% predict(x_test)
```



\newpage 


## LIME

The Lasso model provides a global sense of the influence of the predictors. However if the data structure is complex it might not explain well the interpretation of particular predictions.  

LIME (Local interpretable model-agnostic explanations) is a model-agnostic interpretability model that aims to explain better individual predictions by assuming that the data structure is linear around particular inputs. This technique simulates data around the input values by permuting predictor variables so there is enough data to fit a linear model localy.  

_glmnet_ is not supported by the LIME library (lime::?model_type).  
Since LIME is model-agnostic and the keras model has almost 100% accuracy we'll use it for interpretation of the predictions.  

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(lime)

#class(gmodel())

#?model_type

# Setup of lime::model_type()
model_type.keras.engine.sequential.Sequential <- function(x, ...) {"classification"}

# Setup of lime::predict_model()
predict_model.keras.engine.sequential.Sequential <- function (x, newdata, type, ...) {
  pred <- predict(object = x, x = as.matrix(newdata))
  data.frame(BRCA = pred[,1], COAD = pred[,2], KIRC = pred[,3], LUAD = pred[,4], PRAD = pred[,5])
}

predict_model (x = gmodel, 
               newdata = as.data.frame(x_train), 
               type    = 'raw') %>%
tibble::as_tibble()

# will be used to create hte local model
explainer <- lime(
  x = as.data.frame(x_train),
  model = gmodel, 
  bin_continuous = FALSE
)

#class(explainer)
#summary(explainer)

```

The predictions of data points of category PRAD are analysed:  

```{r echo=TRUE, message=FALSE, warning=FALSE, fig.width=8, fig.height=8}
set.seed(1)
datapoints_index = which(labels[-training_index] == "PRAD")[1:4]
#datapoints_index = sort(output[,5],index.return=T, decreasing = TRUE)$ix[1:2]

explanation_PRAD <- lime::explain(
  x = as.data.frame(x_test)[datapoints_index,],
  explainer = explainer, 
  n_permutations = 10000,
  #dist_fun = "euclidean", #?dist()
  #kernel_width = 0.75,
  feature_select = "lasso_path",
  n_features = 10,
  n_labels = 1
)

plot_features(explanation_PRAD)
```


\newpage 


**Observations:**  

Each time the LIME model is run, the selected coefficients explaining the outputs are different (unless a seed is set before execution). The reason for this could come from:  

* Not enough number of permutations for the large number of predictors we have (>20000), leading to the "simulated" data to be very different each time for the same data point. I tried increasing _n_permutations_ from 5000 (default) to 25000 but the results keep changing (and not enough RAM - 32 GB - to increase the value more than that; takes very long too).  

* High variance and low correlation between predictors as seen in the PCA analysis. $R^2$ of the local models ("explanation fit" in the LIME plots) is very low for all the data points analysed.  

Different data points for the same category are explained by different predictors too.  

\  

Common genes between Lasso selected predictors and LIME explanation predictors:

```{r echo=TRUE, message=FALSE}
# 0 BRCA
# 1 COAD
# 2 KIRC
# 3 LUAD
# 4 PRAD

# no lasso selected predictor among all the predictors returned by LIME
lime_feat = as.vector(explanation_PRAD[["feature"]])
lasso_feat = colnames(x_lasso_test[,coeffs[["4"]]@i])
intersect(lasso_feat,lime_feat)
```

Reasons for this could be:  

* A poor LIME model as described above.  

* The more influential genes explaining individual predictions are different from the genes selected by Lasso. Having high variance but non-overlapping categories could lead to this? As a large variety of predictors are able to classify well all the categories, so small differences in the input will change which are the more influential predictors, Lasso only reflecting on "aggregate".  






